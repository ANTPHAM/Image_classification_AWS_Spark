{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   #   Building a classification model for images by using Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to present the plan of this project.\n",
    "\n",
    "In this notebook, we will be demonstrating how to realize this project on local. \n",
    "\n",
    "This approach  will be a good starting point for the next step of the project concerning deploying the project on AWS.\n",
    "\n",
    "Once all images loaded on S3, we will be using Python scrip to interact with the data and to train + evaluate models. The training and evaluating tasks will be done using 3 services in AWS : EC2, EMR and S3.\n",
    "\n",
    "- Step 1. Loading raw image data:\n",
    "    \n",
    "The aim is to extract all images form a .tar file and to put them into different folders so the task of feature extraction  can be done smotthly\n",
    "    \n",
    "- Step 2. Feature extraction:\n",
    " \n",
    " We will be taking avantage of using the pre-trained model VGG16 in Keras to do this job. \n",
    " \n",
    " Notice that the VGG16 model will put an image through the trained network, so we will be ale to choose the layer from which the extraction will be started.\n",
    " \n",
    " The features will be stored in folders following the label of the sample so that classification models can interact with easily.\n",
    " \n",
    "- Step3. Model training:\n",
    "\n",
    "We will be still applying the SVM algorithm provided in MLib of SPark. We will be approaching 2 mode of clsassification : One vs One and One vs All on different classes and different size of train/test set as well.\n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Etape 1.  Collection des images:\n",
    "    \n",
    " Le but est d'extraire toutes les images du fichier initial (sous forme de \".tar.gz\") et de les ranger dans de    \n",
    " differents dossiers  de maniere a ce que l'extraction des donnees puisse se faire facilement.  \n",
    "    \n",
    "\n",
    " - Etape 2.  Extraction des 'features' a partir des donnees image:\n",
    "\n",
    "Nous allons tirer avantage  du modele \"pre-trained\" VGG16 afin d'extraire les \"features\"  des images.\n",
    "\n",
    "Pour rappel, le modele VGG16 prend une image en entree  pour la passer dans son reseau comprenant plusieurs couches de \n",
    "parametres. Nous pouvons choisir la couche a partir de laquelle les parametres du modele vont s'appliquer sur l'image.  \n",
    "\n",
    "Les \"features\" vont etre gardes dans les dossiers afin que les modeles Machine Learning puissent les utiliser \n",
    "corectement.\n",
    "\n",
    "  - Etape 3. Apprentissage du modele:\n",
    "        \n",
    "Nous allons appliquer l'algorithme SVM de la library MLib dans le context Spark pour entrainer et evaluer des modeles. Apres avoir pousse les images sur AWS S3, nous allons interagir avec ces donnees par les scrips de Python pour entrainer les modeles. L'entrainement et l'evaluation des modeles se font en utilisant le service EMR de AWS pour beneficier Spark/ Hadoop sur AWS. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import tarfile \n",
    "import re \n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "from split import split_on_rawdata, split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all images in a folder and then extract the name of each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootPath = os.getcwd()\n",
    "image_path = os.path.join(rootPath,'images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tarf = tarfile.open(\"images.tar.gz\")\n",
    "filenamesIntarf = tarf.getnames()\n",
    "\n",
    "image_list=[]\n",
    "for img in filenamesIntarf[1:]:\n",
    "    img = img.partition('/')[2]\n",
    "    image_list.append(img)\n",
    "if not os.path.exists(image_path):\n",
    "    tarf.extractall()\n",
    "    tarf.close()\n",
    "else:\n",
    "    if len(os.listdir(image_path)) == len(image_list):\n",
    "        pass\n",
    "tarf.close()\n",
    "\n",
    "# get the name of each class\n",
    "label=[]\n",
    "for img in image_list:\n",
    "    image_name=re.split('\\d',img)[0].rstrip('_')\n",
    "    if image_name not in label:\n",
    "        label.append(image_name)\n",
    "    else:\n",
    "        pass  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels in the data set: boxer, chihuahua, pug, english_setter, english_cocker_spaniel, samoyed, Egyptian_Mau, german_shorthaired, Ragdoll, British_Shorthair, american_pit_bull_terrier, beagle, american_bulldog, shiba_inu, Abyssinian, Siamese, saint_bernard, leonberger, Maine_Coon, Birman, miniature_pinscher, Bombay, wheaten_terrier, Sphynx, scottish_terrier, newfoundland, Persian, staffordshire_bull_terrier, Bengal, keeshond, japanese_chin, great_pyrenees, Russian_Blue, havanese, basset_hound, yorkshire_terrier, pomeranian\n"
     ]
    }
   ],
   "source": [
    "print('Labels in the data set: ' + ', '.join(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 2 folders called respectivement \"train\" and 'test', split images into 2 parts at a thresold  move them into these folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder, test_folder = [image_path + str(i) for i in ['\\\\train','\\\\test']]\n",
    "for f in train_folder, test_folder:\n",
    "    for lb in label:\n",
    "        subf=str('\\\\' + lb)\n",
    "        if not os.path.exists(f+subf):\n",
    "            os.makedirs(f+subf)\n",
    "image_gr=[]\n",
    "for lb in label:\n",
    "    cl=[]\n",
    "    image_gr.append(cl)\n",
    "    for img in image_list:\n",
    "        if re.split('\\d',img)[0].rstrip('_')== lb:\n",
    "            cl.append(img)\n",
    "        else:\n",
    "            pass    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine a thresold making the ratio between traning set and test set\n",
    "\n",
    "For example : 0.6 will be dividing the data set with the ratio 60% train set/40% test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please select a tradeoff from 0 to 1 to split data into the train/test set (exp. 0.6) : \n",
      "0.6\n"
     ]
    }
   ],
   "source": [
    "print(\"Please select a tradeoff from 0 to 1 to split data into the train/test set (exp. 0.6) : \")\n",
    "split_cutoff = float(input())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_on_rawdata(cutoff = None):\n",
    "    if cutoff ==  None:\n",
    "        split_cutoff = float(0.8)\n",
    "    else:\n",
    "        split_cutoff = cutoff    \n",
    "    try:\n",
    "        for gr in image_gr:\n",
    "            for i in range(len(gr)):\n",
    "                class_name = re.split('\\d',gr[i])[0].rstrip('_')\n",
    "                class_name =\"\\\\\" + class_name + \"\\\\\"\n",
    "                train_images, test_images = os.listdir(train_folder + class_name), os.listdir(test_folder + class_name)\n",
    "                if (len(os.listdir(image_path)) > 2):\n",
    "                    if i < split_cutoff*len(gr): # for each class, take tradeoff*number of examples of this class\n",
    "                        if not os.path.isfile(train_folder + class_name + gr[i]):\n",
    "                            os.rename(image_path + \"\\\\\" + gr[i], train_folder + class_name + gr[i])\n",
    "                    else:\n",
    "                        if not os.path.isfile(test_folder + class_name + gr[i]):\n",
    "                            os.rename(image_path + '\\\\' + gr[i], test_folder + class_name + gr[i])    \n",
    "                else:\n",
    "                    if len(train_images) < int(split_cutoff*len(gr)):\n",
    "                        l = random.sample(test_images,int(split_cutoff*len(gr)) - len(train_images))\n",
    "                        for f in l:\n",
    "                            if not os.path.isfile(train_folder + class_name + f):\n",
    "                                os.rename(test_folder + class_name + f, train_folder + class_name + f)\n",
    "                    elif len(train_images) > int(split_cutoff*len(gr)):\n",
    "                        l = random.sample(train_images, len(train_images) - int(split_cutoff*len(gr)))\n",
    "                        for f in l:\n",
    "                            if not os.path.isfile(test_folder + class_name + f):\n",
    "                                os.rename(train_folder + class_name + f, test_folder + class_name + f)\n",
    "                    else:\n",
    "                        pass  \n",
    "                if i == 1:\n",
    "                    print(class_name, len(train_images), len(test_images))\n",
    "                else:\n",
    "                    pass  \n",
    "    except:\n",
    "        print(\"no image to move, please load data!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\boxer\\ 120 80\n",
      "\\chihuahua\\ 120 80\n",
      "\\pug\\ 120 80\n",
      "\\english_setter\\ 120 80\n",
      "\\english_cocker_spaniel\\ 120 80\n",
      "\\samoyed\\ 120 80\n",
      "\\Egyptian_Mau\\ 120 80\n",
      "\\german_shorthaired\\ 120 80\n",
      "\\Ragdoll\\ 120 80\n",
      "\\British_Shorthair\\ 120 80\n",
      "\\american_pit_bull_terrier\\ 120 80\n",
      "\\beagle\\ 120 80\n",
      "\\american_bulldog\\ 120 80\n",
      "\\shiba_inu\\ 120 80\n",
      "\\Abyssinian\\ 121 82\n",
      "\\Siamese\\ 120 80\n",
      "\\saint_bernard\\ 120 80\n",
      "\\leonberger\\ 120 80\n",
      "\\Maine_Coon\\ 120 80\n",
      "\\Birman\\ 120 80\n",
      "\\miniature_pinscher\\ 120 80\n",
      "\\Bombay\\ 120 80\n",
      "\\wheaten_terrier\\ 120 80\n",
      "\\Sphynx\\ 120 80\n",
      "\\scottish_terrier\\ 119 80\n",
      "\\newfoundland\\ 120 80\n",
      "\\Persian\\ 120 80\n",
      "\\staffordshire_bull_terrier\\ 114 77\n",
      "\\Bengal\\ 120 80\n",
      "\\keeshond\\ 120 80\n",
      "\\japanese_chin\\ 120 80\n",
      "\\great_pyrenees\\ 120 80\n",
      "\\Russian_Blue\\ 120 80\n",
      "\\havanese\\ 120 80\n",
      "\\basset_hound\\ 120 80\n",
      "\\yorkshire_terrier\\ 120 80\n",
      "\\pomeranian\\ 120 80\n"
     ]
    }
   ],
   "source": [
    "split_on_rawdata(cutoff = split_cutoff )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature extraction using a pre-trained model within Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will be using the model VGG16 to extract features from images. Let's load this model (as described in https://arxiv.org/abs/1409.1556)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Initialize the model and determine the layer we will start at.\n",
    "\n",
    "- We have decided to take the model for the layer 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Eric\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "=================================================================\n",
      "Total params: 134,260,544\n",
      "Trainable params: 134,260,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"fc...)`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "base_model = VGG16(weights='imagenet')\n",
    "# Model will produce the output of the 'fc2'layer which is the penultimate neural network layer\n",
    "# (see the paper above for mode details)\n",
    "model = Model(input=base_model.input, output=base_model.get_layer('fc2').output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Loading pyspark and diferent required modules that we will be using to build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.ml.linalg import SparseVector, DenseVector\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.mllib.classification import SVMWithSGD, SVMModel\n",
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\spark\\\\spark-2.4.3-bin-hadoop2.7'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init('C:\\spark\\spark-2.4.3-bin-hadoop2.7')\n",
    "sc = SparkContext('local')\n",
    "spark = SparkSession(sc)\n",
    "os.environ.get(\"SPARK_HOME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load model VGG16 as described in https://arxiv.org/abs/1409.1556"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a function to extract features from images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(model, image_path, mode = None):\n",
    "    try:\n",
    "        img = image.load_img(image_path, target_size=(224, 224))\n",
    "        x = image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        x = preprocess_input(x)\n",
    "        features = model.predict(x)\n",
    "        if mode == 'array':\n",
    "            return features[0]\n",
    "        elif mode == 'list':\n",
    "            return features.tolist()[0]\n",
    "        else:\n",
    "            return features[0]\n",
    "    except:\n",
    "        pass   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define paths to folders and sub-folders where images have been stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = image_path\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = [\"\".join(str(i)) for i in os.listdir(train_dir)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels in the data set: Abyssinian, american_bulldog, american_pit_bull_terrier, basset_hound, beagle, Bengal, Birman, Bombay, boxer, British_Shorthair, chihuahua, Egyptian_Mau, english_cocker_spaniel, english_setter, german_shorthaired, great_pyrenees, havanese, japanese_chin, keeshond, leonberger, Maine_Coon, miniature_pinscher, newfoundland, Persian, pomeranian, pug, Ragdoll, Russian_Blue, saint_bernard, samoyed, scottish_terrier, shiba_inu, Siamese, Sphynx, staffordshire_bull_terrier, wheaten_terrier, yorkshire_terrier\n"
     ]
    }
   ],
   "source": [
    "print('Labels in the data set: ' + ', '.join(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In order to store features extracted from images, we'll be creating 2 subfoldes \"train\" and \"test\" where features ,which are json format files, will be put into"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(rootPath +'\\\\features'):\n",
    "    for f in ['train','test']:\n",
    "        os.makedirs('features\\\\%s' %f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define paths to theses folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extraction_path = os.path.join(rootPath, \"features\")\n",
    "feature_extraction_train, feature_extraction_test = [feature_extraction_path + str(i) for i in ['\\\\train','\\\\test']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extraction_train, feature_extraction_test = [feature_extraction_path + str(i) for i in ['\\\\train','\\\\test']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a function allowing to extract features from each image and store these feature\n",
    "\n",
    "- Extract features of each image, save results under lists in json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractAll():\n",
    "    for folder in os.listdir(train_dir):\n",
    "        if not os.path.exists(feature_extraction_train +'\\\\' + folder):\n",
    "            os.makedirs(\"features\\\\train\\\\%s\" %folder)\n",
    "        for img in os.listdir(train_dir + \"\\\\\" + folder):\n",
    "            img_path = os.path.join(train_dir + \"\\\\\" + folder,img)\n",
    "            imgname = ''.join(img.split('.')[:-1]) # remove .jpeg\n",
    "            features = extract_features(img_path, mode ='list')\n",
    "            filepath = feature_extraction_train + \"\\\\\" + folder + '\\\\'+ imgname + \".json\"\n",
    "            if not os.path.isfile(filepath):\n",
    "                with open(filepath, 'w') as out:\n",
    "                    json.dump(features, out)        \n",
    "    \n",
    "    for folder in os.listdir(validation_dir):\n",
    "        if not os.path.exists(feature_extraction_test +'\\\\' + folder):\n",
    "            os.makedirs(\"features\\\\test\\\\%s\" %folder)\n",
    "        for img in os.listdir(validation_dir + \"\\\\\" + folder):\n",
    "            img_path = os.path.join(validation_dir + \"\\\\\" + folder,img)\n",
    "            imgname = ''.join(img.split('.')[:-1]) # remove .jpeg\n",
    "            features = extract_features(img_path, mode = 'list')\n",
    "            filepath = feature_extraction_test + \"\\\\\" + folder +'\\\\'+ imgname + \".json\"\n",
    "            if not os.path.isfile(filepath):\n",
    "                with open(filepath, 'w') as out:\n",
    "                    json.dump(features, out)   "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "extractAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a function allowwing to balance between the train and the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(feature_extraction_train,feature_extraction_test,cutoff = None):\n",
    "    if cutoff ==  None:\n",
    "        split_cutoff = float(0.8)\n",
    "    else:\n",
    "        split_cutoff = cutoff\n",
    "    \n",
    "    for folder in os.listdir(feature_extraction_train):\n",
    "        feature_extraction_train_folder = os.path.join(feature_extraction_train,folder)\n",
    "        feature_extraction_test_folder = os.path.join(feature_extraction_test,folder)\n",
    "        train_images = os.listdir(feature_extraction_train_folder)\n",
    "        test_images = os.listdir(feature_extraction_test_folder)\n",
    "        gr = train_images + test_images\n",
    "        #D[folder] = (len(train_images), len(test_images))\n",
    "        if len(train_images) < int(split_cutoff*len(gr)):\n",
    "            l = random.sample(test_images,int(split_cutoff*len(gr)) - len(train_images))\n",
    "            for f in l:\n",
    "                if not os.path.isfile(feature_extraction_train_folder + \"\\\\\"+ f):\n",
    "                    os.rename(feature_extraction_test_folder + \"\\\\\"+ f, feature_extraction_train_folder + \"\\\\\"+ f)\n",
    "                    \n",
    "        else :\n",
    "            l = random.sample(train_images, len(train_images) - int(split_cutoff*len(gr)))\n",
    "            for f in l:\n",
    "                if not os.path.isfile(feature_extraction_test_folder + \"\\\\\"+ f):\n",
    "                    os.rename(feature_extraction_train_folder + \"\\\\\"+ f, feature_extraction_test_folder + \"\\\\\"+ f)\n",
    "    \n",
    "    D = {}\n",
    "    for folder in os.listdir(feature_extraction_train):\n",
    "        train_images = os.listdir(feature_extraction_train_folder)\n",
    "        test_images = os.listdir(feature_extraction_test_folder)\n",
    "        D[folder] = (len(train_images), len(test_images)) \n",
    "        \n",
    "    return D\n",
    "   \n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Building model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can select one of two methods to train a binary classification model: One vs All or One vs One.\n",
    "\n",
    "\n",
    "- As this is a simulation work, we let you the possibility to choose different parameters for building a model, such as the thresold, One vs All or One vs One based model, the classes considered as the object of a classification problem.\n",
    "\n",
    "\n",
    "- Let's create 2 functions that will be enabling us to load features , transform them into a structure so the Mlib SVM can work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadAll(filename):\n",
    "    '''\n",
    "    filename = the name of an unique file , for instance'yorkshire_terrier_98.json'\n",
    "    '''\n",
    "    class1_train=[]\n",
    "    class0_train=[]\n",
    "    class1_test=[]\n",
    "    class0_test=[]\n",
    "    try:\n",
    "        folder = re.split('\\d',filename)[0].rstrip('_')\n",
    "        with open(feature_extraction_train + '\\\\' + folder + '\\\\' + filename) as json_data:\n",
    "            ctr = json.load(json_data) \n",
    "            if filename.startswith(class1):\n",
    "                class1_train.append(ctr)\n",
    "                \n",
    "            else:\n",
    "                class0_train.append(ctr)\n",
    "    except:\n",
    "        folder = re.split('\\d',filename)[0].rstrip('_')\n",
    "        with open(feature_extraction_test + '\\\\' + folder + '\\\\' + filename) as json_data:\n",
    "            ctr = json.load(json_data) \n",
    "            if filename.startswith(class1):\n",
    "                class1_test.append(ctr)\n",
    "                \n",
    "            else:\n",
    "                class0_test.append(ctr)\n",
    "                \n",
    "    return filename,class1_train, class0_train, class1_test, class0_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertAll():\n",
    "    json_trainfilenames = []\n",
    "    json_testfilenames = []\n",
    "    for folder in os.listdir(feature_extraction_train):\n",
    "        feature_extraction_train_folder = os.path.join(feature_extraction_train,folder)\n",
    "        feature_extraction_test_folder = os.path.join(feature_extraction_test,folder)\n",
    "        trainfiles, testfiles = os.listdir(feature_extraction_train_folder), os.listdir(feature_extraction_test_folder)\n",
    "        if trainmode.lower() == 'yes':\n",
    "            json_trainfilenames.append(trainfiles[::2])\n",
    "            json_testfilenames.append(testfiles[::2])\n",
    "            \n",
    "        else:\n",
    "            if (folder == class1) or (folder == class0) :\n",
    "                json_trainfilenames.append(trainfiles[::2])\n",
    "                json_testfilenames.append(testfiles[::2])\n",
    "            else:\n",
    "                pass \n",
    "    json_trainfilenames = [''.join(n) for i in range(len(json_trainfilenames)) for n in json_trainfilenames[i]]\n",
    "    json_testfilenames = [''.join(n) for i in range(len(json_testfilenames)) for n in json_testfilenames[i]]\n",
    "    \n",
    "    # we are going to use RDD oject to handle features\n",
    "    rdd_trainfilenames = sc.parallelize(json_trainfilenames)\n",
    "    rdd_testfilenames = sc.parallelize(json_testfilenames)    \n",
    "    trainRDD = rdd_trainfilenames.map(loadAll)\n",
    "    testRDD = rdd_testfilenames.map(loadAll)\n",
    "    train_features = trainRDD.collect()\n",
    "    test_features = testRDD.collect()\n",
    "    \n",
    "    class1_train = [train_features[i][1][0] for i in range(len(train_features))\n",
    "                if ((len(train_features[i][1]) !=0) and (train_features[i][1][0] is not None))]# remove Null and NoneType elements\n",
    "    class0_train = [train_features[i][2][0] for i in range(len(train_features))\n",
    "                    if ((len(train_features[i][2]) !=0) and (train_features[i][2][0] is not None))]\n",
    "    class1_test = [test_features[i][3][0] for i in range(len(test_features))\n",
    "                   if ((len(test_features[i][3]) !=0) and (test_features[i][3][0] is not None))]\n",
    "    class0_test = [test_features[i][4][0] for i in range(len(test_features))\n",
    "                   if ((len(test_features[i][4]) !=0) and (test_features[i][4][0] is not None))]\n",
    "    return class1_train, class0_train, class1_test, class0_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are you looking to train a model based on method 'One vs All'? \n",
      "\n",
      "If so,  please enter 'Yes', otherwise enter 'No' \n",
      "no\n"
     ]
    }
   ],
   "source": [
    "print(\"Are you looking to train a model based on method 'One vs All'? \")\n",
    "print(\"\\nIf so,  please enter 'Yes', otherwise enter 'No' \")\n",
    "trainmode = str(input())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please choose one of the following labels as the target class : \n",
      "\n",
      "Abyssinian,american_bulldog,american_pit_bull_terrier,basset_hound,beagle,Bengal,Birman,Bombay,boxer,British_Shorthair,chihuahua,Egyptian_Mau,english_cocker_spaniel,english_setter,german_shorthaired,great_pyrenees,havanese,japanese_chin,keeshond,leonberger,Maine_Coon,miniature_pinscher,newfoundland,Persian,pomeranian,pug,Ragdoll,Russian_Blue,saint_bernard,samoyed,scottish_terrier,shiba_inu,Siamese,Sphynx,staffordshire_bull_terrier,wheaten_terrier,yorkshire_terrier\n",
      "\n",
      "Siamese\n"
     ]
    }
   ],
   "source": [
    "print(\"Please choose one of the following labels as the target class : \\n\" +'\\n' + ','.join(label))\n",
    "print('')\n",
    "class1 = str(input())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sphynx\n"
     ]
    }
   ],
   "source": [
    "if trainmode.lower() == 'no':\n",
    "    print('')\n",
    "    class0 = str(input())\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please select a tradeoff from 0 to 1 to split data into the train/test set (exp. 0.6) : \n",
      "0.6\n"
     ]
    }
   ],
   "source": [
    "print(\"Please select a tradeoff from 0 to 1 to split data into the train/test set (exp. 0.6) : \")\n",
    "split_cutoff = float(input())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Split data into train set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "spl = split(feature_extraction_train,feature_extraction_test,cutoff = split_cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Siamese (120, 80)\n"
     ]
    }
   ],
   "source": [
    "print(class1,spl[class1])\n",
    "print(class0, spl[class0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Transform and put features to the related class before feeding to a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = convertAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class1_train, class0_train, class1_test, class0_test = inputs[0], inputs[1], inputs[2], inputs[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(class0_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 SVMWithSGD ( mlib.Classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In order to train a SVM model, we need first to handle original inputs , there by these can be explored by the model.\n",
    "\n",
    "- We will be using the function Labeledpoint to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbtrainvec1 = [LabeledPoint(1,class1_train[i]) for i in range(len(class1_train))]\n",
    "lbtrainvec0 = [LabeledPoint(0,class0_train[i]) for i in range(len(class0_train))]\n",
    "\n",
    "lbtestvec1 = [LabeledPoint(1,class1_test[i]) for i in range(len(class1_test))]\n",
    "lbtestvec0 = [LabeledPoint(0,class0_test[i]) for i in range(len(class0_test))]\n",
    "    \n",
    "lbtrainvec = lbtrainvec1 + lbtrainvec0 \n",
    "lbtestvec = lbtestvec1 + lbtestvec0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_svm = SVMWithSGD.train(sc.parallelize(lbtrainvec), iterations=100, regParam = 0.1, regType = 'l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_svm = model_svm.predict(sc.parallelize(class1_test + class0_test)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_svm=np.array(prediction_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytest = np.append(np.ones(len(class1_test)),np.zeros(len(class0_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trained a SVM model based on method 'OnevsOne'\n",
      "\n",
      "Label 1: Siamese ; Lablel 0: Sphynx\n",
      "\n",
      "Train/Test: 0.6/0.4\n",
      "\n",
      "Accuracy : 98.75%\n"
     ]
    }
   ],
   "source": [
    "if trainmode.lower() == 'yes':\n",
    "    print(\"\\nTrained a SVM model based on method 'OnevsAll'\")\n",
    "    print(\"\\nLabel 1 %s ; Lablel 0 : all other classses\" % class1)\n",
    "    print('\\nTrain/Test: {}/{}'.format(split_cutoff, round((1-split_cutoff),2)))\n",
    "else:\n",
    "    print(\"\\nTrained a SVM model based on method 'OnevsOne'\")\n",
    "    print(\"\\nLabel 1: {} ; Lablel 0: {}\".format(class1,class0))\n",
    "    print('\\nTrain/Test: {}/{}'.format(split_cutoff, round((1-split_cutoff),2)))\n",
    "    \n",
    "print ('\\nAccuracy : %.2f' % float((np.dot(ytest,prediction_svm.T) + np.dot(1-ytest,1-prediction_svm.T))/float(ytest.size)*100) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the below cde if need to save the model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "svmmodel_path = os.path.join(os.getcwd(),\"modelSvm\")\n",
    "\n",
    "#model_svm.write().overwrite().save(sc,svmmodel_path)\n",
    "if not os.path.exists(svmmodel_path):\n",
    "    model_svm.save(sc, svmmodel_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code can be used to make predictions "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from pyspark.mllib.classification import SVMWithSGD, SVMModel\n",
    "Svm_model=SVMModel.load(sc, Svm_model_path)\n",
    "Svm_model.predict(new_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 Put all together\n",
    "\n",
    "We are going to simulate the training task with different parameters such as names of classes and levels of cutoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "iteration 1\n",
      "Ragdoll (60, 140)\n",
      "Siamese (60, 140)\n",
      "Trained a SVM model based on method 'OnevsOne'\n",
      "Label 1: Ragdoll ; Lablel 0: Siamese\n",
      "Train/Test: 0.3/0.7\n",
      "Accuracy : 97.14%\n",
      "\n",
      "iteration 1\n",
      "Ragdoll (60, 140)\n",
      "Persian (60, 140)\n",
      "Trained a SVM model based on method 'OnevsOne'\n",
      "Label 1: Ragdoll ; Lablel 0: Persian\n",
      "Train/Test: 0.3/0.7\n",
      "Accuracy : 91.43%\n",
      "\n",
      "iteration 1\n",
      "Ragdoll (60, 140)\n",
      "Egyptian_Mau (60, 140)\n",
      "Trained a SVM model based on method 'OnevsOne'\n",
      "Label 1: Ragdoll ; Lablel 0: Egyptian_Mau\n",
      "Train/Test: 0.3/0.7\n",
      "Accuracy : 100.00%\n",
      "\n",
      "iteration 1\n",
      "Siamese (60, 140)\n",
      "Persian (60, 140)\n",
      "Trained a SVM model based on method 'OnevsOne'\n",
      "Label 1: Siamese ; Lablel 0: Persian\n",
      "Train/Test: 0.3/0.7\n",
      "Accuracy : 99.29%\n",
      "\n",
      "iteration 1\n",
      "Siamese (60, 140)\n",
      "Egyptian_Mau (60, 140)\n",
      "Trained a SVM model based on method 'OnevsOne'\n",
      "Label 1: Siamese ; Lablel 0: Egyptian_Mau\n",
      "Train/Test: 0.3/0.7\n",
      "Accuracy : 98.57%\n",
      "\n",
      "iteration 1\n",
      "Persian (60, 140)\n",
      "Egyptian_Mau (60, 140)\n",
      "Trained a SVM model based on method 'OnevsOne'\n",
      "Label 1: Persian ; Lablel 0: Egyptian_Mau\n",
      "Train/Test: 0.3/0.7\n",
      "Accuracy : 100.00%\n",
      "\n",
      "iteration 1\n",
      "Ragdoll (120, 80)\n",
      "Siamese (120, 80)\n",
      "Trained a SVM model based on method 'OnevsOne'\n",
      "Label 1: Ragdoll ; Lablel 0: Siamese\n",
      "Train/Test: 0.6/0.4\n",
      "Accuracy : 100.00%\n",
      "\n",
      "iteration 1\n",
      "Ragdoll (120, 80)\n",
      "Persian (120, 80)\n",
      "Trained a SVM model based on method 'OnevsOne'\n",
      "Label 1: Ragdoll ; Lablel 0: Persian\n",
      "Train/Test: 0.6/0.4\n",
      "Accuracy : 91.25%\n",
      "\n",
      "iteration 1\n",
      "Ragdoll (120, 80)\n",
      "Egyptian_Mau (120, 80)\n",
      "Trained a SVM model based on method 'OnevsOne'\n",
      "Label 1: Ragdoll ; Lablel 0: Egyptian_Mau\n",
      "Train/Test: 0.6/0.4\n",
      "Accuracy : 100.00%\n",
      "\n",
      "iteration 1\n",
      "Siamese (120, 80)\n",
      "Persian (120, 80)\n",
      "Trained a SVM model based on method 'OnevsOne'\n",
      "Label 1: Siamese ; Lablel 0: Persian\n",
      "Train/Test: 0.6/0.4\n",
      "Accuracy : 100.00%\n",
      "\n",
      "iteration 1\n",
      "Siamese (120, 80)\n",
      "Egyptian_Mau (120, 80)\n",
      "Trained a SVM model based on method 'OnevsOne'\n",
      "Label 1: Siamese ; Lablel 0: Egyptian_Mau\n",
      "Train/Test: 0.6/0.4\n",
      "Accuracy : 100.00%\n",
      "\n",
      "iteration 1\n",
      "Persian (120, 80)\n",
      "Egyptian_Mau (120, 80)\n",
      "Trained a SVM model based on method 'OnevsOne'\n",
      "Label 1: Persian ; Lablel 0: Egyptian_Mau\n",
      "Train/Test: 0.6/0.4\n",
      "Accuracy : 98.75%\n",
      "\n",
      "iteration 1\n",
      "Ragdoll (180, 20)\n",
      "Siamese (180, 20)\n",
      "Trained a SVM model based on method 'OnevsOne'\n",
      "Label 1: Ragdoll ; Lablel 0: Siamese\n",
      "Train/Test: 0.9/0.1\n",
      "Accuracy : 95.00%\n",
      "\n",
      "iteration 1\n",
      "Ragdoll (180, 20)\n",
      "Persian (180, 20)\n",
      "Trained a SVM model based on method 'OnevsOne'\n",
      "Label 1: Ragdoll ; Lablel 0: Persian\n",
      "Train/Test: 0.9/0.1\n",
      "Accuracy : 90.00%\n",
      "\n",
      "iteration 1\n",
      "Ragdoll (180, 20)\n",
      "Egyptian_Mau (180, 20)\n",
      "Trained a SVM model based on method 'OnevsOne'\n",
      "Label 1: Ragdoll ; Lablel 0: Egyptian_Mau\n",
      "Train/Test: 0.9/0.1\n",
      "Accuracy : 100.00%\n",
      "\n",
      "iteration 1\n",
      "Siamese (180, 20)\n",
      "Persian (180, 20)\n",
      "Trained a SVM model based on method 'OnevsOne'\n",
      "Label 1: Siamese ; Lablel 0: Persian\n",
      "Train/Test: 0.9/0.1\n",
      "Accuracy : 100.00%\n",
      "\n",
      "iteration 1\n",
      "Siamese (180, 20)\n",
      "Egyptian_Mau (180, 20)\n",
      "Trained a SVM model based on method 'OnevsOne'\n",
      "Label 1: Siamese ; Lablel 0: Egyptian_Mau\n",
      "Train/Test: 0.9/0.1\n",
      "Accuracy : 100.00%\n",
      "\n",
      "iteration 1\n",
      "Persian (180, 20)\n",
      "Egyptian_Mau (180, 20)\n",
      "Trained a SVM model based on method 'OnevsOne'\n",
      "Label 1: Persian ; Lablel 0: Egyptian_Mau\n",
      "Train/Test: 0.9/0.1\n",
      "Accuracy : 100.00%\n"
     ]
    }
   ],
   "source": [
    "random.seed(1)\n",
    "cl = ['Ragdoll', 'Siamese','Persian', 'Egyptian_Mau' ]\n",
    "c1 =[]\n",
    "c0 = []\n",
    "train_test = []\n",
    "accuracy = []\n",
    "for split_cutoff in [0.3, 0.6, 0.9]:\n",
    "    spl = split(feature_extraction_train,feature_extraction_test, split_cutoff)\n",
    "    for j in range(len(cl)):\n",
    "        class1 = cl[j]\n",
    "        i = 1\n",
    "        for class0 in cl[j+1:]:\n",
    "            print(\"\\niteration %d\" %i)\n",
    "            print(class1,spl[class1])\n",
    "            print(class0, spl[class0])\n",
    "            train_mode = 'no'\n",
    "            inputs = convertAll()\n",
    "            class1_train, class0_train, class1_test, class0_test = inputs[0], inputs[1], inputs[2], inputs[3]\n",
    "            lbtrainvec1 = [LabeledPoint(1,class1_train[i]) for i in range(len(class1_train))]\n",
    "            lbtrainvec0 = [LabeledPoint(0,class0_train[i]) for i in range(len(class0_train))]\n",
    "            lbtestvec1 = [LabeledPoint(1,class1_test[i]) for i in range(len(class1_test))]\n",
    "            lbtestvec0 = [LabeledPoint(0,class0_test[i]) for i in range(len(class0_test))]\n",
    "            lbtrainvec = lbtrainvec1 + lbtrainvec0 \n",
    "            lbtestvec = lbtestvec1 + lbtestvec0\n",
    "            model_svm = SVMWithSGD.train(sc.parallelize(lbtrainvec), iterations=100, regParam = 0.1, regType = 'l2')\n",
    "            prediction_svm = model_svm.predict(sc.parallelize(class1_test + class0_test)).collect()\n",
    "            prediction_svm=np.array(prediction_svm)\n",
    "            ytest = np.append(np.ones(len(class1_test)),np.zeros(len(class0_test)))\n",
    "            if trainmode.lower() == 'yes':\n",
    "                print(\"Trained a SVM model based on method 'OnevsAll'\")\n",
    "                print(\"Label 1 %s ; Lablel 0 : all other classses\" % class1)\n",
    "                print('Train/Test: {}/{}'.format(split_cutoff, round((1-split_cutoff),2)))\n",
    "            else:\n",
    "                print(\"Trained a SVM model based on method 'OnevsOne'\")\n",
    "                print(\"Label 1: {} ; Lablel 0: {}\".format(class1,class0))\n",
    "                print('Train/Test: {}/{}'.format(split_cutoff, round((1-split_cutoff),2)))\n",
    "            acc = '%.2f' %float((np.dot(ytest,prediction_svm.T) + np.dot(1-ytest,1-prediction_svm.T))/float(ytest.size)*100) \n",
    "            c1.append(class1)\n",
    "            c0.append(class0)\n",
    "            train_test.append('{}/{}'.format(split_cutoff, round((1-split_cutoff),2)))\n",
    "            accuracy.append(acc)\n",
    "\n",
    "            print ('Accuracy : %s' % acc + '%')\n",
    "        i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class1</th>\n",
       "      <th>class0</th>\n",
       "      <th>0.3/0.7</th>\n",
       "      <th>0.6/0.4</th>\n",
       "      <th>0.9/0.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Persian</td>\n",
       "      <td>Egyptian_Mau</td>\n",
       "      <td>100.00</td>\n",
       "      <td>98.75</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ragdoll</td>\n",
       "      <td>Egyptian_Mau</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ragdoll</td>\n",
       "      <td>Persian</td>\n",
       "      <td>91.43</td>\n",
       "      <td>91.25</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ragdoll</td>\n",
       "      <td>Siamese</td>\n",
       "      <td>97.14</td>\n",
       "      <td>100.00</td>\n",
       "      <td>95.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Siamese</td>\n",
       "      <td>Egyptian_Mau</td>\n",
       "      <td>98.57</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Siamese</td>\n",
       "      <td>Persian</td>\n",
       "      <td>99.29</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    class1        class0  0.3/0.7  0.6/0.4  0.9/0.1\n",
       "0  Persian  Egyptian_Mau   100.00    98.75    100.0\n",
       "1  Ragdoll  Egyptian_Mau   100.00   100.00    100.0\n",
       "2  Ragdoll       Persian    91.43    91.25     90.0\n",
       "3  Ragdoll       Siamese    97.14   100.00     95.0\n",
       "4  Siamese  Egyptian_Mau    98.57   100.00    100.0\n",
       "5  Siamese       Persian    99.29   100.00    100.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "table = pd.DataFrame(list(zip(c1,c0,train_test,accuracy)), columns = ['class1','class0','train/test','acc(%)'])\n",
    "table = table.pivot_table(index = ['class1','class0'], columns = 'train/test', values ='acc(%)', aggfunc='first').reset_index()\n",
    "table.to_csv('result.csv', index = False)\n",
    "pd.read_csv('result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Logistic Regression ( ml.Classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to try out to implement a Logistic Regression taking inputs that need to be stored by data frame in spark context ( when using ml.classification and not mlib.classification).\n",
    "\n",
    "We will be using the function DenseVector  to turn data lists into vectors in a spark context, then the function  createDataFrame to make dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext('local')\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainvec1 = [(1,DenseVector(class1_train[i])) for i in range(len(class1_train))]\n",
    "trainvec0 = [(0,DenseVector(class0_train[i])) for i in range(len(class0_train))]\n",
    "trainvec = trainvec1 + trainvec0\n",
    "testvec1 = [(1,DenseVector(class1_test[i])) for i in range(len(class1_test))]\n",
    "testvec0 = [(0,DenseVector(class0_test[i])) for i in range(len(class0_test))]\n",
    "testvec= testvec1 + testvec0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf = spark.createDataFrame(trainvec, [\"label\", \"features\"])\n",
    "\n",
    "testdf = spark.createDataFrame(testvec, [\"label\", \"features\"])\n",
    "\n",
    "lr = LogisticRegression(maxIter=100, regParam=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn a LogisticRegression model. This uses the parameters stored in lr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lr.fit(traindf)\n",
    "\n",
    "prediction = model.transform(testdf)\n",
    "\n",
    "out_lr = prediction.select(\"label\",\"probability\", \"prediction\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label=1 -> prob=[2.2305797509017952e-06,0.999997769420249], prediction=1.0\n"
     ]
    }
   ],
   "source": [
    "for row in out_lr[::20]:\n",
    "    print(\"label=%s -> prob=%s, prediction=%s\"\n",
    "          % (row.label, row.probability, row.prediction))\n",
    "\n",
    "Y=[]\n",
    "pred = []\n",
    "for row in out_lr:\n",
    "    Y.append(row[0])\n",
    "    pred.append(row[2])\n",
    "Y = np.array(Y)\n",
    "pred = np.array(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained a Logistic regression model based on method 'OnevsOne'\n",
      "label 1: Egyptian_Mau  lablel 2: Egyptian_Mau\n",
      "Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "if trainmode.lower() == 'yes':\n",
    "    print(\"trained a Logistic regression model based on method 'OnevsAll'\")\n",
    "    print(\"label 1 %s  lablel 2 : all other classses\" % class1)\n",
    "else:\n",
    "    print(\"trained a Logistic regression model based on method 'OnevsOne'\")\n",
    "    print(\"label 1: {}  lablel 2: {}\".format(class1,class0))\n",
    "print('Accuracy: %.2f' % float((np.dot(Y,pred.T) + np.dot(1-Y,1-pred.T))/float(Y.size)*100) + '%')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "lgrmodel_path = os.path.join(os.getcwd())+ \"\\\\modelLogReg\"\n",
    "\n",
    "if not os.path.exists(lgrmodel_path):\n",
    "    model.write().overwrite().save(lgrmodel_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
